<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <title>Sanyam Jain</title>
  <link rel="icon" href="path/to/favicon.ico" type="image/x-icon">
  <style>
    body {
      font-family: Arial, sans-serif;
      background-color: #f1f1f1;
      margin: 0;
      padding: 0;
    }
    
    header {
      background-color: #333;
      color: #fff;
      padding: 20px;
      text-align: center;
    }
    
    h1 {
      margin: 0;
      font-size: 30px;
      letter-spacing: 2px;
    }
    
    main {
      max-width: 800px;
      margin: 20px auto;
      padding: 20px;
      background-color: #fff;
      box-shadow: 0 0 10px rgba(0, 0, 0, 0.1);
    }
    
    p {
      line-height: 1.6;
    }
    
    .tab-container {
      margin-bottom: 20px;
    }
    
    .tab-links {
      display: flex;
      flex-wrap: wrap;
      justify-content: center;
    }
    
    .tab-link {
      padding: 10px 20px;
      background-color: #333;
      color: #fff;
      text-decoration: none;
      border-top-left-radius: 5px;
      border-top-right-radius: 5px;
      margin: 5px;
    }
    
    .tab-link.active {
      background-color: #666;
    }
    
    .tab-content {
      display: none;
      padding: 20px;
      background-color: #fff;
      box-shadow: 0 0 10px rgba(0, 0, 0, 0.1);
    }
    
    .tab-content.active {
      display: block;
    }
    
    .affiliation-placeholder {
      text-align: center;
    }
    
    .row-section {
      margin-bottom: 20px;
      padding: 10px;
      border-radius: 5px;
    }
    
    .row-section:hover {
      border: 1px solid #ccc;
      cursor: pointer;
    }
    
    .row-section img {
      max-width: 100%;
      max-height: 350px;
      display: block;
      margin: 0 auto;
    }
    
    .row-section p {
      text-align: justify;
    }
    
    .paper-title {
      font-weight: bold;
      margin-bottom: 5px;
    }
    
    .paper-authors {
      color: #666;
      margin-bottom: 10px;
    }
    
    .paper-abstract {
      line-height: 1.4;
    }
    
    .google-scholar-link {
      display: inline-block;
      padding: 5px 10px;
      background-color: #337ab7;
      color: #fff;
      text-decoration: none;
      border-radius: 3px;
    }
    
    footer {
      background-color: #333;
      color: #fff;
      padding: 20px;
      text-align: center;
    }
    
    footer p {
      margin: 0;
      font-size: 14px;
    }
    
    /* Responsive Styles */
    @media only screen and (max-width: 600px) {
      main {
        padding: 10px;
      }
      
      .tab-links {
        flex-direction: column;
        align-items: center;
      }
      
      .tab-link {
        margin: 5px 0;
      }
      
      .affiliation-placeholder {
        margin-top: 20px;
      }
    }
    
    .popup {
      display: none;
      position: fixed;
      left: 50%;
      top: 50%;
      transform: translate(-50%, -50%);
      background-color: #fff;
      padding: 20px;
      box-shadow: 0 0 10px rgba(0, 0, 0, 0.3);
      z-index: 9999;
    }
    
    .popup-overlay {
      display: none;
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      height: 100%;
      background-color: rgba(0, 0, 0, 0.5);
      z-index: 9998;
    }
    
    .popup-content {
      margin-bottom: 20px;
    }
    
    .popup-title {
      font-weight: normal;
      margin-bottom: 5px;
      font-size: 18px;
    }
    
    .popup-close {
      position: absolute;
      top: 10px;
      right: 10px;
      cursor: pointer;
    }

  .projects-container {
  display: flex;
  flex-wrap: wrap;
  justify-content: space-between;
}

.project-box {
  width: 100%;
  margin-bottom: 20px;
  padding: 10px;
  border-radius: 5px;
  background-color: #fff;
  box-shadow: 0 0 10px rgba(0, 0, 0, 0.1);
  transition: transform 0.3s ease, box-shadow 0.3s ease;
}

.project-box:hover {
  transform: translateY(-5px);
  box-shadow: 0 5px 10px rgba(0, 0, 0, 0.2);
}

.project-box img {
  max-width: 100%;
  max-height: 350px;
  display: block;
  margin: 0 auto;
}

.project-box p {
  text-align: justify;
}

.project-title {
  font-weight: bold;
  margin-bottom: 5px;
}

.project-description {
  color: #666;
  margin-bottom: 10px;
}

.project-link {
  display: inline-block;
  padding: 5px 10px;
  background-color: #337ab7;
  color: #fff;
  text-decoration: none;
  border-radius: 3px;
}
.publications-container {
    display: flex;
    flex-wrap: wrap;
    justify-content: space-between;
  }

  .publication-box {
    width: 100%;
    margin-bottom: 20px;
    padding: 10px;
    border-radius: 5px;
    background-color: #fff;
    box-shadow: 0 0 10px rgba(0, 0, 0, 0.1);
    transition: transform 0.3s ease, box-shadow 0.3s ease;
  }

  .publication-box:hover {
    transform: translateY(-5px);
    box-shadow: 0 5px 10px rgba(0, 0, 0, 0.2);
  }

  .publication-box p {
    text-align: justify;
  }

  .publication-title {
    font-weight: bold;
    margin-bottom: 5px;
  }

  .publication-authors {
    color: #666;
    margin-bottom: 10px;
  }

  .publication-abstract {
    line-height: 1.4;
  }
 
  /* Add your CSS styles here */
  .tab-content {
      margin: 20px;
    }
    

    
    .experience-item {
      margin-bottom: 20px;
    }
    
    .experience-title {
      font-weight: bold;
      margin-bottom: 5px;
    }
    
    .experience-duration {
      color: #999;
      margin-bottom: 5px;
    }
    
    .experience-description {
      margin-top: 5px;
    }
 /* Add your CSS styles here */
 .tab-content {
      margin: 20px;
    }
    

    
    .experience-item {
      border: 2px solid #ccc;
        padding: 10px;
        margin-bottom: 20px;
        transition: box-shadow 0.3s ease;
    }
    
    .experience-title {
      font-weight: bold;
      margin-bottom: 5px;
    }
    
    .experience-duration {
      color: #999;
      margin-bottom: 5px;
    }
    
    .experience-description {
      margin-top: 5px;
    }
    .experience-item:hover {
        box-shadow: 0 0 5px rgba(0, 0, 0, 0.3);
      }

      .education-section {
  background-color: #f5f5f5;
  border-radius: 10px;
  padding: 20px;
  margin-bottom: 20px;
  box-shadow: 0 2px 4px rgba(0, 0, 0, 0.1);
  transition: background-color 0.3s ease;
}

.education-section h3 {
  color: #333;
  margin-top: 0;
}

.education-section p {
  color: #666;
  margin-bottom: 10px;
}

.education-section:hover {
  background-color: #eaeaea;
}
.education-section .read-more {
    display: none;
  }

  #contact {
  background-color: #f7f7f7;
  padding: 20px;
}

#contact h2 {
  color: #333;
  font-size: 24px;
  margin-bottom: 10px;
}

#contact p {
  color: #555;
  margin-bottom: 20px;
}

.contact-links {
  display: flex;
  flex-wrap: wrap;
}

.contact-link {
  color: #333;
  text-decoration: none;
  transition: color 0.3s ease;
  background-color: #fff;
  border: 2px solid #777;
  border-radius: 5px;
  padding: 10px 15px;
  margin-right: 10px;
  margin-bottom: 10px;
  font-size: 16px;
  font-weight: bold;
  text-transform: uppercase;
}

.contact-link:hover {
  color: #999;
  background-color: #f2f2f2;
}



  </style>
  <script>
    function openTab(evt, tabName) {
      var i, tabContent, tabLink;
      
      tabContent = document.getElementsByClassName("tab-content");
      for (i = 0; i < tabContent.length; i++) {
        tabContent[i].style.display = "none";
      }
      
      tabLink = document.getElementsByClassName("tab-link");
      for (i = 0; i < tabLink.length; i++) {
        tabLink[i].className = tabLink[i].className.replace(" active", "");
      }
      
      document.getElementById(tabName).style.display = "block";
      evt.currentTarget.className += " active";
    }

    function openLinkInNewTab(url) {
      var isSection = ["about", "publications", "projects", "affiliations", "experience", "contact"].includes(url);
      if (!isSection) {
        window.open(url, "_blank");
      }
    }

    function openPublicationPopup(title, authors, abstract) {
      var popup = document.getElementById("publicationPopup");
      var overlay = document.getElementById("publicationOverlay");
      var popupTitle = document.getElementById("popupTitle");
      var popupAuthors = document.getElementById("popupAuthors");
      var popupAbstract = document.getElementById("popupAbstract");

      popupTitle.textContent = title;
      popupAuthors.textContent = authors;
      popupAbstract.textContent = abstract;

      popup.style.display = "block";
      overlay.style.display = "block";
    }

    function closePublicationPopup() {
      var popup = document.getElementById("publicationPopup");
      var overlay = document.getElementById("publicationOverlay");

      popup.style.display = "none";
      overlay.style.display = "none";
    }
      // JavaScript code to toggle "read more" section
  document.addEventListener("DOMContentLoaded", function() {
    const toggleButtons = document.querySelectorAll(".toggle-read-more");
    toggleButtons.forEach(function(button) {
      button.addEventListener("click", function(event) {
        event.preventDefault();
        const readMoreSection = this.previousElementSibling;
        readMoreSection.style.display = readMoreSection.style.display === "none" ? "block" : "none";
        this.textContent = this.textContent === "Read More" ? "Read Less" : "Read More";
      });
    });
  });
  </script>
</head>
<body>
  <header>
    <h1>Sanyam Jain</h1>
  </header>
  
  <main>
    <div class="tab-container">
      <div class="tab-links">
        <a href="#" class="tab-link active" onclick="openTab(event, 'about')">About</a>
        <a href="#" class="tab-link" onclick="openTab(event, 'publications')">Publications</a>
        <a href="#" class="tab-link" onclick="openTab(event, 'projects')">Projects</a>
        <a href="#" class="tab-link" onclick="openTab(event, 'affiliations')">Affi.</a>
        <a href="#" class="tab-link" onclick="openTab(event, 'experience')">Experience</a>
        <a href="#" class="tab-link" onclick="openTab(event, 'education')">Education</a>
        <a href="#" class="tab-link" onclick="openTab(event, 'contact')">Contact</a>
      </div>
      
      <div id="about" class="tab-content active">
        <h2>About Me</h2>
        <div class="row-section">
          <img src="https://raw.githubusercontent.com/s4nyam/s4nyam.github.io/main/mine.JPG" alt="About Me Image">
          <p>
            I am currently a Master's student (2022-24) at <a href="https://www.hiof.no/">Østfold University College</a> and a summer Intern (2023) at <a href="https://www.oslomet.no/en/about/employee/stenic/">OsloMet University</a>, Norway. My research focus belongs to ALife, Evolution, Complexity and Artificial General Intelligence. I enjoy working with Prof <a href="https://www.nichele.eu/">Stefano Nichele</a> (also supervisor) where we together study complexity and emergent behaviour in discrete Cellular Automata, continuous CA (Lenia) and Neural CA. Some of my previous work and projects also determine my interest in Deep Learning for Computer Vision and Explainable AI. I finished my Junior Research Fellowship (and did not continue PhD) at Indian Institute of Technology (IIT) Jodhpur (2020-2022) where I studied Deep Learning, Classical ML, and Dependable AI taught by Dr. <a href="http://home.iitj.ac.in/~mvatsa/">Mayank Vatsa</a> and Dr. <a href="http://home.iitj.ac.in/~richa/">Richa Singh</a>. Conversely, I was also part of <a href="https://vanets-iitj.github.io/pages/people.html">VANETs lab</a> supervised by Prof <a href="https://research.iitj.ac.in/researcher/debasis-das-2">Debasis Das</a>, where my focus was to provide Machine Learning edge for the team. Moreover, I was part of <a href="https://iitj.ac.in/qic/">QIC group</a> at the IIT as well. I finished my early Computer Science Engineering (Bachelor's 2015-2019) degree from <a href="https://www.upes.ac.in/">UPES, Dehradun</a> at the foothills of Himalayas. Most of my guidance during bachelor's for which I am thankful, always to Dr <a href="https://www.ravitomar.in/">Ravi Tomar</a>. I belong to the central part of India, from a town called Banswara which lies in south Rajasthan.
          </p>
        </div>
      </div>
      
      <div id="publications" class="tab-content">
        <h2>Publications / Preprints - <a href="https://scholar.google.com/citations?user=R9XAK2IAAAAJ&hl=en" class="google-scholar-link">Google Scholar</a></h2>
       
        <!-- <div class="row-section" onclick="openPublicationPopup('This paper implements and investigates popular adversarial attacks on the YOLOv5 Object Detection algorithm. The paper explores the vulnerability of the YOLOv5 to adversarial attacks in the context of traffic and road sign detection. The paper investigates the impact of different types of attacks, including the Limited memory Broyden Fletcher Goldfarb Shanno (L-BFGS), the Fast Gradient Sign Method (FGSM) attack, the Carlini and Wagner (C&W) attack, the Basic Iterative Method (BIM) attack, the Projected Gradient Descent (PGD) attack, One Pixel Attack, and the Universal Adversarial Perturbations attack on the accuracy of YOLOv5 in detecting traffic and road signs. The results show that YOLOv5 is susceptible to these attacks, with misclassification rates increasing as the magnitude of the perturbations increases. We also explain the results using saliency maps. The findings of this paper have important implications for the safety and reliability of object detection algorithms used in traffic and transportation systems, highlighting the need for more robust and secure models to ensure their effectiveness in real-world applications.')">
          <p class="paper-abstract">
            Jain, S. (2023). Adversarial Attack On Yolov5 For Traffic And Road Sign Detection. arXiv preprint arXiv:2306.06071.
          </p>
        </div> -->
        <div class="publications-container">
          <div class="publication-box" onclick="openPublicationPopup('This paper implements and investigates popular adversarial attacks on the YOLOv5 Object Detection algorithm. The paper explores the vulnerability of the YOLOv5 to adversarial attacks in the context of traffic and road sign detection. The paper investigates the impact of different types of attacks, including the Limited memory Broyden Fletcher Goldfarb Shanno (L-BFGS), the Fast Gradient Sign Method (FGSM) attack, the Carlini and Wagner (C&W) attack, the Basic Iterative Method (BIM) attack, the Projected Gradient Descent (PGD) attack, One Pixel Attack, and the Universal Adversarial Perturbations attack on the accuracy of YOLOv5 in detecting traffic and road signs. The results show that YOLOv5 is susceptible to these attacks, with misclassification rates increasing as the magnitude of the perturbations increases. We also explain the results using saliency maps. The findings of this paper have important implications for the safety and reliability of object detection algorithms used in traffic and transportation systems, highlighting the need for more robust and secure models to ensure their effectiveness in real-world applications.')">
            <p class="publication-abstract">
              <span class="publication-title">Jain, S. (2023). Adversarial Attack On Yolov5 For Traffic And Road Sign Detection. arXiv preprint arXiv:2306.06071.</span><br>
              <span class="publication-authors">Authors: Sanyam Jain</span>
            </p>
          </div>

          <div class="publication-box" onclick="openPublicationPopup('Marine animals and deep underwater objects are difficult to recognize and monitor for safety of aquatic life. There is an increasing challenge when the water is saline with granular particles and impurities. In such natural adversarial environment, traditional approaches like CNN start to fail and are expensive to compute. This project involves implementing and evaluating various object detection models, including EfficientDet, YOLOv5, YOLOv8, and Detectron2, on an existing annotated underwater dataset, called the Brackish-Dataset. The dataset comprises annotated image sequences of fish, crabs, starfish, and other aquatic animals captured in Limfjorden water with limited visibility. The aim of this research project is to study the efficiency of newer models on the same dataset and contrast them with the previous results based on accuracy and inference time. Firstly, I compare the results of YOLOv3 (31.10% mean Average Precision (mAP)), YOLOv4 (83.72% mAP), YOLOv5 (97.6%), YOLOv8 (98.20%), EfficientDet (98.56% mAP) and Detectron2 (95.20% mAP) on the same dataset. Secondly, I provide a modified BiSkFPN mechanism (BiFPN neck with skip connections) to perform complex feature fusion in adversarial noise which makes modified EfficientDet robust to perturbations. Third, analyzed the effect on accuracy of EfficientDet (98.63% mAP) and YOLOv5 by adversarial learning (98.04% mAP). Last, I provide class activation map based explanations (CAM) for the two models to promote Explainability in black box models. Overall, the results indicate that modified EfficientDet achieved higher accuracy with five-fold cross validation than the …')">
            <p class="publication-abstract">
              <span class="publication-title">Jain, S. (2023). DeepSeaNet: Improving Underwater Object Detection using EfficientDet. arXiv preprint arXiv:2306.06075</span><br>
              <span class="publication-authors">Authors: Sanyam Jain</span>
            </p>
          </div>

        <!-- <div class="row-section" onclick="openPublicationPopup('This research project investigates Lenia, an artificial life platform that simulates ecosystems of digital creatures. Lenias ecosystem consists of simple, artificial organisms that can move, consume, grow, and reproduce. The platform is important as a tool for studying artificial life and evolution, as it provides a scalable and flexible environment for creating a diverse range of organisms with varying abilities and behaviors. Measuring complexity in Lenia is a key aspect of the study, which identifies the metrics for measuring long-term complex emerging behavior of rules, with the aim of evolving better Lenia behaviors which are yet not discovered. The Genetic Algorithm uses neighborhoods or kernels as genotype while keeping the rest of the parameters of Lenia as fixed, for example growth function, to produce different behaviors respective to the population and then measures fitness value to decide the complexity of the resulting behavior. First, we use Variation over Time as a fitness function where higher variance between the frames are rewarded. Second, we use Auto-encoder based fitness where variation of the list of reconstruction loss for the frames is rewarded. Third, we perform combined fitness where higher variation of the pixel density of reconstructed frames is rewarded. All three experiments are tweaked with pixel alive threshold and frames used. Finally, after performing nine experiments of each fitness for 500 generations, we pick configurations from all experiments such that there is a scope of further evolution, and run it for 2500 generations. Results show that the kernels center of mass increases with a specific set of pixels and together...')">
          <p class="paper-abstract">
            Jain, S., Shrestha, A., Nichele, S. (2023). Capturing Emerging Complexity in Lenia. arXiv preprint arXiv:2305.09378.
          </p>
        </div> -->
        <div class="publication-box" onclick="openPublicationPopup('This research project investigates Lenia, an artificial life platform that simulates ecosystems of digital creatures. Lenias ecosystem consists of simple, artificial organisms that can move, consume, grow, and reproduce. The platform is important as a tool for studying artificial life and evolution, as it provides a scalable and flexible environment for creating a diverse range of organisms with varying abilities and behaviors. Measuring complexity in Lenia is a key aspect of the study, which identifies the metrics for measuring long-term complex emerging behavior of rules, with the aim of evolving better Lenia behaviors which are yet not discovered. The Genetic Algorithm uses neighborhoods or kernels as genotype while keeping the rest of the parameters of Lenia as fixed, for example growth function, to produce different behaviors respective to the population and then measures fitness value to decide the complexity of the resulting behavior. First, we use Variation over Time as a fitness function where higher variance between the frames are rewarded. Second, we use Auto-encoder based fitness where variation of the list of reconstruction loss for the frames is rewarded. Third, we perform combined fitness where higher variation of the pixel density of reconstructed frames is rewarded. All three experiments are tweaked with pixel alive threshold and frames used. Finally, after performing nine experiments of each fitness for 500 generations, we pick configurations from all experiments such that there is a scope of further evolution, and run it for 2500 generations. Results show that the kernels center of mass increases with a specific set of pixels and together...')">
          <p class="publication-abstract">
            <span class="publication-title">Jain, S., Shrestha, A., Nichele, S. (2023). Capturing Emerging Complexity in Lenia. arXiv preprint arXiv:2305.09378.</span><br>
            <span class="publication-authors">Authors: Sanyam Jain, Aarati Shreshtha, Stefano Nichele</span>
          </p>
        </div>

        <!-- <div class="row-section" onclick="openPublicationPopup('Training machine learning models in an incremental fashion is not only important but also an efficient way to achieve artificial general intelligence. The ability that humans possess of continuous or lifelong learning helps them to not forget previously learned tasks. However, current neural network models are prone to catastrophic forgetting when it comes to continual learning. Many researchers have come up with several techniques in order to reduce the effect of forgetting from neural networks, however, all techniques are studied classically with a very less focus on changing the machine learning model architecture. In this research paper, we show that it is not only possible to circumvent catastrophic forgetting in continual learning with novel hybrid classical-quantum neural networks, but also explains what features are most important to learn for classification. In addition, we also claim that if the model is trained with these explanations, it tends to give better performance and learn specific features that are far from the decision boundary. Finally, we present the experimental results to show comparisons between classical and classical-quantum hybrid architectures on benchmark MNIST and CIFAR-10 datasets. After successful runs of learning procedure, we found hybrid neural network outperforms classical one in terms of remembering the right evidences of the class-specific features.')">
          <p class="paper-abstract">
            Jain, S. (2023). CQural: A Novel CNN based Hybrid Architecture for Quantum Continual Machine Learning. arXiv preprint arXiv:2305.09738.
          </p>
        </div> -->

        <div class="publication-box" onclick="openPublicationPopup('Training machine learning models in an incremental fashion is not only important but also an efficient way to achieve artificial general intelligence. The ability that humans possess of continuous or lifelong learning helps them to not forget previously learned tasks. However, current neural network models are prone to catastrophic forgetting when it comes to continual learning. Many researchers have come up with several techniques in order to reduce the effect of forgetting from neural networks, however, all techniques are studied classically with a very less focus on changing the machine learning model architecture. In this research paper, we show that it is not only possible to circumvent catastrophic forgetting in continual learning with novel hybrid classical-quantum neural networks, but also explains what features are most important to learn for classification. In addition, we also claim that if the model is trained with these explanations, it tends to give better performance and learn specific features that are far from the decision boundary. Finally, we present the experimental results to show comparisons between classical and classical-quantum hybrid architectures on benchmark MNIST and CIFAR-10 datasets. After successful runs of learning procedure, we found hybrid neural network outperforms classical one in terms of remembering the right evidences of the class-specific features.')">
          <p class="publication-abstract">
            <span class="publication-title">Jain, S. (2023). CQural: A Novel CNN based Hybrid Architecture for Quantum Continual Machine Learning. arXiv preprint arXiv:2305.09738.</span><br>
            <span class="publication-authors">Authors: Sanyam Jain</span>
          </p>
        </div>

        <!-- <div class="row-section" onclick="openPublicationPopup('For a long time, detecting hand gestures and recognizing them as letters or numbers has been a challenging task. This creates communication barriers for individuals with disabilities. This paper introduces a new dataset, the Annotated Dataset for Danish Sign Language (ADDSL). Annota-tions for the dataset were made using the open-source tool LabelImg in the YOLO format. Using this dataset, a one-stage ob-ject detector model (YOLOv5) was trained with the CSP-DarkNet53 backbone and YOLOv3 head to recognize letters (A-Z) and numbers (0-9) using only seven unique images per class (without augmen-tation). Five models were trained with 350 epochs, resulting in an average inference time of 9.02ms per image and a best accu-racy of 92% when compared to previous research. Our results show that modified model is efficient and more accurate than existing work in the same field. The code repository for our model is available at the GitHub repository.')">
          <p class="paper-abstract">
            Jain, S. (2023). ADDSL: Hand Gesture Detection and Sign Language Recognition on Annotated Danish Sign Language. arXiv preprint arXiv:2305.09736.
          </p>
        </div> -->
        <div class="publication-box" onclick="openPublicationPopup('For a long time, detecting hand gestures and recognizing them as letters or numbers has been a challenging task. This creates communication barriers for individuals with disabilities. This paper introduces a new dataset, the Annotated Dataset for Danish Sign Language (ADDSL). Annota-tions for the dataset were made using the open-source tool LabelImg in the YOLO format. Using this dataset, a one-stage ob-ject detector model (YOLOv5) was trained with the CSP-DarkNet53 backbone and YOLOv3 head to recognize letters (A-Z) and numbers (0-9) using only seven unique images per class (without augmen-tation). Five models were trained with 350 epochs, resulting in an average inference time of 9.02ms per image and a best accu-racy of 92% when compared to previous research. Our results show that modified model is efficient and more accurate than existing work in the same field. The code repository for our model is available at the GitHub repository.')">
          <p class="publication-abstract">
            <span class="publication-title">Jain, S. (2023). ADDSL: Hand Gesture Detection and Sign Language Recognition on Annotated Danish Sign Language. arXiv preprint arXiv:2305.09736.</span><br>
            <span class="publication-authors">Authors: Sanyam Jain</span>
          </p>
        </div>

        <!-- <div class="row-section" onclick="openPublicationPopup('This research paper presents an experimental approach to using the Reptile algorithm for reinforcement learning to train a neural network to play Super Mario Bros. We implement the Reptile algorithm using the Super Mario Bros Gym library and TensorFlow in Python, creating a neural network model with a single convolutional layer, a flatten layer, and a dense layer. We define the optimizer and use the Reptile class to create an instance of the Reptile meta-learning algorithm. We train the model using multiple tasks and episodes, choosing actions using the current weights of the neural network model, taking those actions in the environment, and updating the model weights using the Reptile algorithm. We evaluate the performance of the algorithm by printing the total reward for each episode. In addition, we compare the performance of the Reptile algorithm approach to two other popular reinforcement learning algorithms, Proximal Policy Optimization (PPO) and Deep Q-Network (DQN), applied to the same Super Mario Bros task. Our results demonstrate that the Reptile algorithm provides a promising approach to few-shot learning in video game AI, with comparable or even better performance than the other two algorithms, particularly in terms of moves vs distance that agent performs for 1M episodes of training. The results shows that best total distance for world 1-2 in the game environment were ~1732 (PPO), ~1840 (DQN) and ~2300 (RAMario). Full code is available at GitHub projects')">
          <p class="paper-abstract">
            Jain, S. (2023). RAMario: Experimental Approach to Reptile Algorithm--Reinforcement Learning for Mario. arXiv preprint arXiv:2305.09655.
          </p>
        </div> -->

        <div class="publication-box" onclick="openPublicationPopup('This research paper presents an experimental approach to using the Reptile algorithm for reinforcement learning to train a neural network to play Super Mario Bros. We implement the Reptile algorithm using the Super Mario Bros Gym library and TensorFlow in Python, creating a neural network model with a single convolutional layer, a flatten layer, and a dense layer. We define the optimizer and use the Reptile class to create an instance of the Reptile meta-learning algorithm. We train the model using multiple tasks and episodes, choosing actions using the current weights of the neural network model, taking those actions in the environment, and updating the model weights using the Reptile algorithm. We evaluate the performance of the algorithm by printing the total reward for each episode. In addition, we compare the performance of the Reptile algorithm approach to two other popular reinforcement learning algorithms, Proximal Policy Optimization (PPO) and Deep Q-Network (DQN), applied to the same Super Mario Bros task. Our results demonstrate that the Reptile algorithm provides a promising approach to few-shot learning in video game AI, with comparable or even better performance than the other two algorithms, particularly in terms of moves vs distance that agent performs for 1M episodes of training. The results shows that best total distance for world 1-2 in the game environment were ~1732 (PPO), ~1840 (DQN) and ~2300 (RAMario). Full code is available at GitHub projects')">
          <p class="publication-abstract">
            <span class="publication-title">Jain, S. (2023). RAMario: Experimental Approach to Reptile Algorithm--Reinforcement Learning for Mario. arXiv preprint arXiv:2305.09655.</span><br>
            <span class="publication-authors">Authors: Sanyam Jain</span>
          </p>
        </div>

        <!-- <div class="row-section" onclick="openPublicationPopup('This research paper consists of the development of a python chatbot integrated with specific packages and dependencies having capabilities to perform various real-life applications which can also be packaged with some integrated systems as a personal assistant. Various Web services are used using their APIs so as to make it more powerful and less dependent on python packages. It operates completely on Linux (Ubuntu) terminal. Python command with the __main__.py file has been used to run it. Terminal then becomes a live Web-based chatbot service with STT and TTS services. Natural language processing is further done by Wit.ai API which token key is set in the code. Each NLP is done by the API and then processed, and crunched text is sent from the wit engine direct to display terminal.')">
          <p class="paper-abstract">
            Jain, S., Sharma, S., & Tomar, R. (2019). Integration of wit API with python coded terminal bot. In Emerging Technologies in Data Mining and Information Security: Proceedings of IEMIS 2018, Volume 3 (pp. 397-406). Springer Singapore.
          </p>
        </div> -->

        <div class="publication-box" onclick="openPublicationPopup('This research paper consists of the development of a python chatbot integrated with specific packages and dependencies having capabilities to perform various real-life applications which can also be packaged with some integrated systems as a personal assistant. Various Web services are used using their APIs so as to make it more powerful and less dependent on python packages. It operates completely on Linux (Ubuntu) terminal. Python command with the __main__.py file has been used to run it. Terminal then becomes a live Web-based chatbot service with STT and TTS services. Natural language processing is further done by Wit.ai API which token key is set in the code. Each NLP is done by the API and then processed, and crunched text is sent from the wit engine direct to display terminal.')">
          <p class="publication-abstract">
            <span class="publication-title">Integration of wit API with python coded terminal bot. In Emerging Technologies in Data Mining and Information Security: Proceedings of IEMIS 2018, Volume 3 (pp. 397-406). Springer Singapore.</span><br>
            <span class="publication-authors">Authors: Jain, S., Sharma, S., & Tomar, R. (2019).</span>
          </p>
        </div>

        <!-- <div class="row-section" onclick="openPublicationPopup('Cybersecurity is very much essential for Mobile Transactions to complete seamlessly. Mobile Commerce (Mcom.) is the very basic transaction type, which is very commonly used (two in five people use mobile as transaction medium). To secure this, there are various technologies used by this research. The four “factors” formally known as Multi-Factor-Authentication are: two of them are Traditional methods (User Login password and One Time Password (aka OTP)) with addition of Geolocation and Facial Recognition. All the data is converted to a text file, which is hidden in an image (using Babushka algorithm). The end-point then decrypts the image using the same algorithm.')">
          <p class="paper-abstract">
            Jain, S., Gautam, R., Sharma, S., Tomar, R., & Choudhury, T. (2021). Four-Factor Authentication with Emerging Cybersecurity for Mobile Transactions. In Innovations in Cyber Physical Systems: Select Proceedings of ICICPS 2020 (pp. 391-399). Springer Singapore.
          </p>
        </div> -->

        <div class="publication-box" onclick="openPublicationPopup('Cybersecurity is very much essential for Mobile Transactions to complete seamlessly. Mobile Commerce (Mcom.) is the very basic transaction type, which is very commonly used (two in five people use mobile as transaction medium). To secure this, there are various technologies used by this research. The four “factors” formally known as Multi-Factor-Authentication are: two of them are Traditional methods (User Login password and One Time Password (aka OTP)) with addition of Geolocation and Facial Recognition. All the data is converted to a text file, which is hidden in an image (using Babushka algorithm). The end-point then decrypts the image using the same algorithm.')">
          <p class="publication-abstract">
            <span class="publication-title">Four-Factor Authentication with Emerging Cybersecurity for Mobile Transactions. In Innovations in Cyber Physical Systems: Select Proceedings of ICICPS 2020 (pp. 391-399). Springer Singapore.</span><br>
            <span class="publication-authors">Jain, S., Gautam, R., Sharma, S., Tomar, R., & Choudhury, T. (2021).</span>
          </p>
        </div>

      </div>
    </div>
      
      <div id="projects" class="tab-content">
        <h2>Projects</h2>
        <div class="projects-container">
          <div class="project-box">
            <img src="https://raw.githubusercontent.com/s4nyam/s4nyam.github.io/main/mncaportalimage.jpg" alt="mnca image">
            <p>
              <span class="project-title">Frequency Histogram and MNCA</span><br>
              <span class="project-description">Studying Complexity and Emerging Behaviour in ECA, Discrete CA and MNCA</span><br>
              <a href="https://s4nyam.github.io/mncaportal/" class="project-link">MNCA portal</a>
            </p>
          </div>
          
         
        </div>
      </div>
      
      
      





      <div id="affiliations" class="tab-content">
        <h2>Affiliations</h2>
        <div class="affiliation-placeholder">
          <img src="https://raw.githubusercontent.com/s4nyam/s4nyam.github.io/main/oslomet.png" alt=" 1 Logo" width="60" height="50">
          <img src="https://raw.githubusercontent.com/s4nyam/s4nyam.github.io/main/hiof.png" alt=" 1 Logo" width="240" height="50">
          <br/>
          <img src="https://raw.githubusercontent.com/s4nyam/s4nyam.github.io/main/iitj.png" alt=" 2 Logo" width="300" height="50">
          <br/>
          <img src="https://raw.githubusercontent.com/s4nyam/s4nyam.github.io/main/iitjqic.png" alt=" 1 Logo" width="300" height="50">
          <br/>
          <img src="https://raw.githubusercontent.com/s4nyam/s4nyam.github.io/main/vanets%20lab.png" alt=" 2 Logo" width="300" height="50">
          <br/>
          <img src="https://raw.githubusercontent.com/s4nyam/s4nyam.github.io/main/upes.png" alt=" 2 Logo" width="100" height="50">
          <img src="https://raw.githubusercontent.com/s4nyam/s4nyam.github.io/main/upesacm.png" alt=" 1 Logo" width="80" height="50">
          <img src="https://raw.githubusercontent.com/s4nyam/s4nyam.github.io/main/fiserv.png" alt=" 2 Logo" width="120" height="50">
          <br/>
          <img src="https://raw.githubusercontent.com/s4nyam/s4nyam.github.io/main/iitindor.png" alt=" 2 Logo" width="80" height="50">
          <img src="https://raw.githubusercontent.com/s4nyam/s4nyam.github.io/main/nora.png" alt=" 2 Logo" width="120" height="50">
          <img src="https://raw.githubusercontent.com/s4nyam/s4nyam.github.io/main/gate.jpg" alt=" 2 Logo" width="100" height="50">
        </div>
      </div>
      







      <div id="experience" class="tab-content">
        <h2>Academic and Professional Experience</h2>
        <p>
          Here, you can find information about my professional experience.
        </p>
        <!-- Add your experience details here -->
      <div class="experience-item">
        <div class="experience-title">Research Asst. Summer Internship - OsloMet</div>
        <div class="experience-duration">July 2023 - current</div>
        <div class="experience-description">
          Continuing research with Stefano Nichele
        </div>
      </div>
      <div class="experience-item">
        <div class="experience-title">Research Asst. - Part Time - Østfold University College</div>
        <div class="experience-duration">September 2022 - June 2023</div>
        <div class="experience-description">
          - Studying and Analyzing Complexity in Discrete Cellular Automata (1D and 2D) using Evolutionary Algorithm
          <br/>
          - Capturing Emerging Complexity in Lenia
        </div>
      </div>


      <div class="experience-item">
        <div class="experience-title">Teaching Assistant, Machine Learning - Winter 2022 - IIT Jodhpur</div>
        <div class="experience-duration">January 2022 - May 2022</div>
        <div class="experience-description">
          Course Instructor: Dr. Richa Singh, IIT Jodhpur Skills: Machine Learning
          </div>
        </div>


        <div class="experience-item">
          <div class="experience-title">Teaching Assistant, Foundations of Quantum Information and Computation - Trimester 3 - IIT Jodhpur</div>
          <div class="experience-duration">August 2021 - December 2021</div>
          <div class="experience-description">
            Course Instructors: Dr. Subhashish Banerjee, IIT Jodhpur Dr. Atul Kumar, IIT Jodhpur Dr. V. Narayan, IIT Jodhpur
            </div>
          </div>
  
  

      <div class="experience-item">
        <div class="experience-title">Research Asst. - Full Time - IIT Jodhpur</div>
        <div class="experience-duration">September 2020 - June 2022</div>
        <div class="experience-description">
          - Course work Machine Learning (by Dr. Gaurav Harit), Deep Learning (by Dr. Mayank Vatsa), Dependable AI (by Dr. Richa Singh), Cryptography (by Dr. Somitra Sanadhya)
          <br/>
          - Part of VANETs Lab supervised by Dr. Debasis Das.
          </div>
        </div>




        <div class="experience-item">
          <div class="experience-title">Amazon Alexa Specialist - Conversational AI - <a href="keyringcorp.in">KeyringCorp</a></div>
          <div class="experience-duration">2019 - 2020</div>
          <div class="experience-description">
            Proudly, an AWS Activate startup founders portfolio for 10 years. Building Great Experiences for Kids using Voice-Driven AI, VUI, and Alexa skills. Conversational AI, Voice User Experience, Interactive Alexa skills, 100k+ customers worldwide, Helping businesses and institutions to design and develop their custom Alexa Skills and Google Actions.
            </div>
          </div>


          <div class="experience-item">
            <div class="experience-title"> RPA UiPath Consultant </div>
            <div class="experience-duration">2019 - 2020</div>
            <div class="experience-description">
              RPA UiPath Advanced Certified, Level 1,2 and 3, UiRPA, UiARD 1. RPA UiPath Learning Plan and Package 2. RPA UiPath Advanced Certification guidance 3. Helping Startups to setup RPA Automation in their daily working flow. 4. Helping Professional with hands on experience to UiPath RPA Studio.
              </div>
            </div>
  

          <div class="experience-item">
            <div class="experience-title"> Software Eng Intern - YellowAnt Inc </div>
            <div class="experience-duration">May 2018 - July 2018</div>
            <div class="experience-description">
              Built CRM platforms using Django and REST APIs. OAuth for Sugar CRM, Zoho CRM and FreshSales.
              </div>
            </div>


          <div class="experience-item">
            <div class="experience-title"> Remote Intern - IBM Watson </div>
            <div class="experience-duration">May 2018 - July 2018</div>
            <div class="experience-description">
              Studying Watson API.
              </div>
            </div>

      </div>






      <div id="education" class="tab-content">
        <h2>Education</h2>

  <div class="education-section">
    <h3>Østfold University College, Norway</h3>
    <p>Degree: Masters in Science</p>
    <p>Major: Applied Computer Science</p>
    <p>Year: 2022-2024</p>
    <p class="read-more">Currently maintaining 4.4/5 - Stduying Machine Learning specialisation with CPS. Projects for the courses adv. ML and Applied CS project includes "DeepSeaNet" and "Complexity in Lenia" which can be found in publications section as part of preprint.</p>
    <a href="#" class="toggle-read-more">Click for more!</a>
  </div>

  <div class="education-section">
    <h3>Indian Institute of Technology, Jodhpur</h3>
    <p>Degree: PhD (Dropped)</p>
    <p>Major: Quantum Information and Computation with focused coursework on Deep Learning</p>
    <p>Year: 2020 - 2022</p>
    <p class="read-more">Grade: 7.8/10, Activities and societies: Part of VANETS LABActivities and societies: Part of VANETS LAB, Courses: Machine Learning, Dr G. Harit AI 1, Dr D. Mishra Graph Theory and Applications, Dr A. Mishra Deep Learning, Dr M. Vatsa Dependable AI, Dr R. Singh (A- grade) Financial Engg., Dr V. Vijay (A- grade) Cryptography, Dr S. Sanadhya ML for Economics, Dr D. Brahma Adv Machine Learning, Dr M. Vatsa (A grade) Cryptography, Course Project Theoretical analysis of QKD protocol BB84 and experimenting related algorithms on qiskit, (Endsem) QKD protocols and Quantum attacks on blockchain. (Attacks on Bitcoin PoW and Attacks on Digital Signatures) ML2, Endsem Project - Analyzing and Improving the Image Quality of StyleGAN (StyleGAN2) CVPR 2020, HELEN EYE DATASET and Clothing Dataset (Fashion Dataset) ML2, Group Project - Random Multimodel Deep Learning for Classification (arXiv:1805.01890) 20NewsGroup and IMDB dataset & CIFAR-10, MNIST Dataset. Dependable AI - Study of Nature of Different Adversarial Attacks and Impact of Different Data Augmentation Techniques on Robustness.</p>
    <a href="#" class="toggle-read-more">Click for more!</a>
  </div>

  <div class="education-section">
    <h3>UPES, Dehradun, India</h3>
    <p>Degree: BTech (Bachelor's in Technology)</p>
    <p>Major: Computer Science Engineering</p>
    <p>Year: 2015 - 2019</p>
    <p class="read-more">Grade: 77.50, Best Project award for Compariso - whatsapp bot to compare e-commerce prices. Top 10 in R.I.S.E. UPES under supervision of Dr. Ravi Tomar sir with project title: KidExa (Team KeyringCorp) Won Best project Award in Project Parliament (2018) college Minor Project 2 for Facial Recognition significance in cybersecurity. (Team KeyringCorp) Won Third Prize for Real-time Traffic Analysis – IBM ICE on campus Hackathon. (Team KeyringCorp) Won Second prize for 4FactorAuthentication in North India Cyber Security Hackathon at WIC Dehradun. (Team KeyringCorp) Developed Python Chatbot using wit NLP engine. (Wit.ai) Developed Automated Key logger using C. (Unix/Py) Studied Digital Logic, Data Structures and Algorithms, Java, Operating Systems, DBMS, Computer Networks, Theory of Computation, Discrete Maths, Information Retrieval and Search engines, Artificial Intelligence, Mobile Applications.</p>
    <a href="#" class="toggle-read-more">Click for more!</a>
  </div>
  <!-- Add more education sections as needed -->

      </div>
      

      <div id="contact" class="tab-content">
        <h2>Contact Me</h2>
        <p>
          You can reach me via email at <a href="mailto:sanyamjaincs@gmail.com">sanyamjaincs@gmail.com</a>. Connect with me on social media through the following profiles:
        </p>
        <!-- Add your contact details here -->
        <div class="contact-links">
          <a class="contact-link" href="https://github.com/s4nyam">GitHub</a>
          <a class="contact-link" href="https://twitter.com/s4nyam">Twitter</a>
          <a class="contact-link" href="https://www.linkedin.com/in/s4nyam/">LinkedIn</a>
          <a class="contact-link" href="https://www.youtube.com/@SanyamJain001">YouTube</a>
          <a class="contact-link" href="https://www.instagram.com/sanyam.no/">Instagram</a>
        </div>
      </div>
      




      
    </div>
  </main>
  
  <footer>
    <p>&copy; 2023 My Personal Webpage. All rights reserved.</p>
  </footer>
  
  <!-- Publication Popup -->
  <div id="publicationPopup" class="popup">
    <h2 class="popup-title" id="popupTitle"></h2>
    <p class="popup-authors" id="popupAuthors"></p>
    <div class="popup-content">
      <p class="popup-abstract" id="popupAbstract"></p>
    </div>
    <span class="popup-close" onclick="closePublicationPopup()">X</span>
  </div>
  <div id="publicationOverlay" class="popup-overlay"></div>
</body>
</html>